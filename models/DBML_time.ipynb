{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device  = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_utils.mydata import MyData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93f8fe26e74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPSM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     def __init__(self, user_size, item_size, word_size, embedding_dim,\\\n\u001b[1;32m      6\u001b[0m                  \u001b[0mmax_query_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_review_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def pp(s, ss):\n",
    "    print(ss)\n",
    "    print(s.shape)\n",
    "class PSM(nn.Module):\n",
    "    def __init__(self, user_size, item_size, word_size, embedding_dim,\\\n",
    "                 max_query_len, max_review_len, batch_size, time_num,\\\n",
    "                 neg_num=5,sample_num=1,transfer_hidden_dim=100,\\\n",
    "                 sigma_parameter=1e0, kl_parameter=1e0, word_parameter=1e0, device=torch.device('cpu')):\n",
    "        super(PSM, self).__init__()\n",
    "        self.user_size = user_size\n",
    "        self.item_size = item_size\n",
    "        self.word_size = word_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_review_len = max_review_len\n",
    "        self.sample_num = sample_num\n",
    "        self.transfer_hidden_dim = transfer_hidden_dim\n",
    "        self.kl_parameter = kl_parameter\n",
    "        self.sigma_parameter = sigma_parameter\n",
    "        self.word_parameter = word_parameter\n",
    "        self.device = device\n",
    "        self.neg_num = neg_num\n",
    "        self.time_num = time_num\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.time_embdding = nn.Embedding(self.time_num, self.embedding_dim)\n",
    "        self.time2mean = nn.Linear(self.embedding_dim*2, self.embedding_dim)\n",
    "        self.user_mean = nn.Embedding(self.user_size, self.embedding_dim, _weight=torch.ones(self.user_size, self.embedding_dim))\n",
    "        self.user_std = nn.Embedding(self.user_size, self.embedding_dim, _weight=torch.zeros(self.user_size, self.embedding_dim))\n",
    "        \n",
    "        self.item_mean = nn.Embedding(self.item_size, self.embedding_dim, _weight=torch.ones(self.item_size, self.embedding_dim))\n",
    "        self.item_std = nn.Embedding(self.item_size, self.embedding_dim, _weight=torch.zeros(self.item_size, self.embedding_dim))\n",
    "        \n",
    "        \n",
    "        self.wordEmbedding_mean = nn.Embedding(self.word_size, self.embedding_dim, padding_idx=0, _weight=torch.ones(self.word_size, self.embedding_dim))\n",
    "        self.wordEmbedding_std = nn.Embedding(self.word_size, self.embedding_dim, padding_idx=0, _weight=torch.zeros(self.word_size, self.embedding_dim))\n",
    "        self.queryLinear = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "    \n",
    "    \n",
    "        self.transfer_linear_u = nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "        self.transfer_linear_i = nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "        self.transfer_linear_ni = nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "        self.transfer_linear_w = nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "        self.transfer_linear_nw = nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "#         self.transfer_linear = {\n",
    "#             \"u\":nn.Linear(self.embedding_dim, self.transfer_hidden_dim),\n",
    "#             \"i\":nn.Linear(self.embedding_dim, self.transfer_hidden_dim),\n",
    "#             \"ni\":nn.Linear(self.embedding_dim, self.transfer_hidden_dim),\n",
    "#             \"w\":nn.Linear(self.embedding_dim, self.transfer_hidden_dim),\n",
    "#             'nw':nn.Linear(self.embedding_dim, self.transfer_hidden_dim)\n",
    "#         }\n",
    "        self.transfer_mean_u = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_mean_i = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_mean_ni = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_mean_w = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_mean_nw = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "#         self.transfer_mean = {\n",
    "#             \"u\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             \"i\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             \"ni\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             'w':nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             'nw':nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "#         }\n",
    "        self.transfer_std_u = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_std_i = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_std_ni = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_std_w = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "        self.transfer_std_nw = nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "#         self.transfer_std = {\n",
    "#             \"u\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             \"i\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             \"ni\":nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             'w':nn.Linear(self.transfer_hidden_dim, self.embedding_dim),\n",
    "#             'nw':nn.Linear(self.transfer_hidden_dim, self.embedding_dim)\n",
    "#         }\n",
    "        \n",
    "\n",
    "        \n",
    "#         self.userDecoder = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "#         self.itemDecoder = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "    '''\n",
    "    (uid, pid_pos, qids_pos, len_pos, time_bin_pos)\n",
    "    [( uid, pid, qids_neg, len_neg, time_bin_pos),..,( uid, pid, qids_neg, len_neg, time_bin_pos)]*neg_sample_num\n",
    "    '''\n",
    "    \n",
    "    def forward(self, user, item_pos, query, query_len, word, word_len, times, items_neg, word_neg):\n",
    "        self.batch_size = user.shape[0]\n",
    "        '''\n",
    "        time embedding\n",
    "        '''\n",
    "        time_laten = self.time_embdding(times).squeeze(1)\n",
    "        pri_time_laten =self.time_embdding(times-torch.tensor(1).to(self.device))\n",
    "        \n",
    "        '''\n",
    "        user\n",
    "        '''\n",
    "        user_mean = self.user_mean(user).squeeze(1) # (batch, out_size)\n",
    "        user_mean_pri = self.time2mean(torch.cat([user_mean, pri_time_laten], 1))\n",
    "        user_mean = self.time2mean(torch.cat([user_mean, time_laten], 1))\n",
    "        \n",
    "        user_std = self.user_std(user).squeeze(1) # (batch, out_size)\n",
    "        user_std_pri = self.time2mean(torch.cat([user_std, pri_time_laten], 1)).mul(0.5).exp()\n",
    "        user_std = self.time2mean(torch.cat([user_std, time_laten], 1)).mul(0.5).exp()\n",
    "        \n",
    "\n",
    "        \n",
    "        '''\n",
    "        query\n",
    "        '''\n",
    "        query = self.get_train_query_tanh_mean(query, query_len)# ((batch, maxQueryLen))) ---> ((batch, embedding)\n",
    "       \n",
    "        \n",
    "        '''\n",
    "        word\n",
    "        '''\n",
    "        word_mean_pos = self.wordEmbedding_mean(word)\n",
    "        word_mean_pos_pri = self.time2mean(torch.cat([word_mean_pos, pri_time_laten.unsqueeze(1).expand_as(word_mean_pos)], 2))\n",
    "        word_mean_pos = self.time2mean(torch.cat([word_mean_pos, time_laten.unsqueeze(1).expand_as(word_mean_pos)], 2))\n",
    "        \n",
    "        word_std_pos = self.wordEmbedding_std(word)\n",
    "        word_std_pos_pri = self.time2mean(torch.cat([word_std_pos, pri_time_laten.unsqueeze(1).expand_as(word_std_pos)], 2)).mul(0.5).exp()\n",
    "        word_std_pos = self.time2mean(torch.cat([word_std_pos, time_laten.unsqueeze(1).expand_as(word_std_pos)], 2)).mul(0.5).exp()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        neg word\n",
    "        '''\n",
    "        word_mean_neg = self.wordEmbedding_mean(word_neg)\n",
    "        word_mean_neg_pri = self.time2mean(torch.cat([word_mean_neg, pri_time_laten.unsqueeze(1).expand_as(word_mean_neg)], 2))\n",
    "        word_mean_neg = self.time2mean(torch.cat([word_mean_neg, time_laten.unsqueeze(1).expand_as(word_mean_neg)], 2))\n",
    "        \n",
    "        word_std_neg = self.wordEmbedding_std(word_neg)\n",
    "        word_std_neg_pri = self.time2mean(torch.cat([word_std_neg, pri_time_laten.unsqueeze(1).expand_as(word_std_neg)], 2)).mul(0.5).exp()\n",
    "        word_std_neg = self.time2mean(torch.cat([word_std_neg, time_laten.unsqueeze(1).expand_as(word_std_neg)], 2)).mul(0.5).exp()    \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        pos product\n",
    "        '''\n",
    "        item_mean_pos = self.item_mean(item_pos).squeeze(1) # (batch, out_size)\n",
    "        item_mean_pos_pri = self.time2mean(torch.cat([item_mean_pos, pri_time_laten], 1))\n",
    "        item_mean_pos = self.time2mean(torch.cat([item_mean_pos, time_laten], 1))\n",
    "        \n",
    "        item_std_pos = self.item_std(item_pos).squeeze(1) # (batch, out_size)\n",
    "        item_std_pos_pri = self.time2mean(torch.cat([item_std_pos, pri_time_laten], 1)).mul(0.5).exp()\n",
    "        item_std_pos = self.time2mean(torch.cat([item_std_pos, time_laten], 1)).mul(0.5).exp()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        neg product\n",
    "        '''\n",
    "        items_mean_neg = self.item_mean(items_neg)# (batch, neg_sample_num, out_size)\n",
    "        items_mean_neg_pri = self.time2mean(torch.cat([items_mean_neg, pri_time_laten.unsqueeze(1).expand_as(items_mean_neg)], 2))\n",
    "        items_mean_neg = self.time2mean(torch.cat([items_mean_neg, time_laten.unsqueeze(1).expand_as(items_mean_neg)], 2))\n",
    "        \n",
    "        items_std_neg = self.item_std(items_neg)# (batch, neg_sample_num, out_size)\n",
    "        items_std_neg_pri = self.time2mean(torch.cat([items_std_neg, pri_time_laten.unsqueeze(1).expand_as(items_std_neg)], 2)).mul(0.5).exp()\n",
    "        items_std_neg = self.time2mean(torch.cat([items_std_neg, time_laten.unsqueeze(1).expand_as(items_std_neg)], 2)).mul(0.5).exp()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        用户和product word的隐变量采样\n",
    "        '''\n",
    "        user_sample = self.reparameter(user_mean, user_std)\n",
    "        product_sample = self.reparameter(item_mean_pos, item_std_pos)\n",
    "        product_sample_neg = self.reparameter(items_mean_neg, items_std_neg)\n",
    "        word_sample = self.reparameter(word_mean_pos, word_std_pos)\n",
    "        word_sample_neg = self.reparameter(word_mean_neg, word_std_neg)\n",
    "        \n",
    "#         query_sample\n",
    "        '''\n",
    "        loss 计算\n",
    "        '''\n",
    "        # 主要的损失u+q-i 采样得到的uqi 计算重构误差\n",
    "        loss_main, dis_pos, dis_neg = self.lossF_sigmod_ml(user_sample, query, product_sample, product_sample_neg)\n",
    "        # 计算uw和iw的损失\n",
    "        user_word_loss = self.word_loss(user_sample, word_sample, word_len, word_sample_neg)\n",
    "        item_word_loss = self.word_loss(product_sample, word_sample, word_len, word_sample_neg)\n",
    "\n",
    "        \n",
    "        # 转移损失(KL损失) -->\n",
    "        # 转移概率 loss current_mean, current_std, prior_mean, prior_std\n",
    "        user_trans_loss = self.transfer_kl_loss(user_mean, user_std, user_mean_pri, user_std_pri, False, 'u')\n",
    "        product_trans_pos_loss = self.transfer_kl_loss(item_mean_pos, item_std_pos, item_mean_pos_pri, item_std_pos_pri, False, 'i')\n",
    "        product_trans_neg_loss = self.transfer_kl_loss(items_mean_neg, items_std_neg, items_mean_neg_pri, items_std_neg_pri, True, 'ni')\n",
    "        word_trans_pos_loss = self.transfer_kl_loss(word_mean_pos, word_std_pos, word_mean_pos_pri, word_std_pos_pri, True, 'w')\n",
    "        word_trans_pos_neg_loss = self.transfer_kl_loss(word_mean_neg, word_std_neg, word_mean_neg_pri, word_std_neg_pri, True, 'nw')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #         query_trans_loss\n",
    "        loss = loss_main+\\\n",
    "        (user_word_loss+item_word_loss)*torch.tensor(self.word_parameter).to(self.device)+\\\n",
    "        (user_trans_loss+product_trans_pos_loss+product_trans_neg_loss+word_trans_pos_loss+word_trans_pos_neg_loss)*\\\n",
    "        torch.tensor(self.kl_parameter).to(self.device)\n",
    "        \n",
    "        loss = (loss, loss_main, user_word_loss+item_word_loss, user_trans_loss+product_trans_pos_loss+product_trans_neg_loss+word_trans_pos_loss+word_trans_pos_neg_loss)\n",
    "        \n",
    "        return user_mean, user_std, query, \\\n",
    "                item_mean_pos, item_std_pos,\\\n",
    "                items_mean_neg, items_std_neg, \\\n",
    "                user_sample, product_sample, product_sample_neg, \\\n",
    "                loss, dis_pos, dis_neg,\\\n",
    "                word_mean_pos, word_std_pos, word_mean_neg, word_std_neg\n",
    "    \n",
    "    def word_loss(self, itemOrUser, word_pos, word_len, word_neg):\n",
    "        len_mask = torch.tensor([ [1.]*int(i.item())+[0.]*(self.max_review_len-int(i.item())) for i in word_len]).unsqueeze(2).to(self.device)\n",
    "        word_pos = word_pos.mul(len_mask)\n",
    "        itemOrUser.unsqueeze_(1)\n",
    "        dis_pos = (itemOrUser - word_pos).norm(2, dim=2).mean(dim=1)\n",
    "        dis_neg = (itemOrUser - word_neg).norm(2, dim=2).mean(dim=1)\n",
    "        wl = torch.log(torch.sigmoid(dis_neg-dis_pos)).mean()*(-1.0)\n",
    "        itemOrUser.squeeze_(1)\n",
    "        return wl\n",
    "        \n",
    "    def reparameter(self, mean, std):\n",
    "#         sigma = torch.exp(torch.mul(0.5,log_var))\n",
    "        std_z = torch.randn(std.shape, device=self.device)\n",
    "        return mean + torch.tensor(self.sigma_parameter).to(self.device)*std* Variable(std_z)  # Reparameterization trick\n",
    "    \n",
    "    \n",
    "    def get_train_query_tanh_mean(self, query, query_len):\n",
    "        '''\n",
    "        input size: (batch, maxQueryLen)\n",
    "        对query处理使用函数\n",
    "        tanh(W*(mean(Q))+b)\n",
    "        \n",
    "        '''\n",
    "        query = self.wordEmbedding_mean(query) # size: ((batch, maxQueryLen))) ---> (batch, len(query[i]), embedding)\n",
    "        # query len mask 使得padding的向量为0\n",
    "        len_mask = torch.tensor([ [1.]*int(i.item())+[0.]*(self.max_query_len-int(i.item())) for i in query_len]).unsqueeze(2).to(self.device)\n",
    "        query = query.mul(len_mask)\n",
    "\n",
    "        query = query.sum(dim=1).div(query_len.unsqueeze(1).float())\n",
    "        query = self.queryLinear(query).tanh()\n",
    "    \n",
    "        return query\n",
    "\n",
    "    def transfer_mlp(self, prior, aim='u'):\n",
    "        transfer_linear = getattr(self, 'transfer_linear_'+aim)\n",
    "        current_hidden = transfer_linear(prior)\n",
    "        transfer_mean = getattr(self, 'transfer_mean_'+aim)\n",
    "        transfer_std = getattr(self, 'transfer_std_'+aim)\n",
    "        return transfer_mean(current_hidden), transfer_std(current_hidden).mul(0.5).exp()\n",
    "\n",
    "    \n",
    "    def transfer_kl_loss(self, current_mean, current_std, prior_mean, prior_std, dim3=False, aim='u'):\n",
    "        dim2 = current_mean.shape[1]\n",
    "        if (dim3 == False):\n",
    "            current_transfer_mean = torch.zeros((self.batch_size, self.embedding_dim), device=self.device)\n",
    "            current_transfer_std = torch.zeros((self.batch_size, self.embedding_dim), device=self.device)\n",
    "            for i in range(self.sample_num):\n",
    "                prior_instance = self.reparameter(prior_mean, prior_std)\n",
    "                cur_instance = self.transfer_mlp(prior_instance, aim)\n",
    "                current_transfer_mean += cur_instance[0]\n",
    "                current_transfer_std += cur_instance[1]\n",
    "\n",
    "            # 取多个采样的Q(Zt-1)分布的均值为最终的loss 计算使用的P(Zt|B1:t-1)分布\n",
    "            current_transfer_mean = current_transfer_mean.div(self.sample_num)\n",
    "            current_transfer_std = current_transfer_std.div(self.sample_num**2)\n",
    "\n",
    "            kl_loss = self.DKL(current_mean, current_std, current_transfer_mean, current_transfer_std)\n",
    "        else:\n",
    "            current_transfer_mean = torch.zeros((self.batch_size, dim2, self.embedding_dim), device=self.device)\n",
    "            current_transfer_std = torch.zeros((self.batch_size, dim2, self.embedding_dim), device=self.device)\n",
    "            for i in range(self.sample_num):\n",
    "                prior_instance = self.reparameter(prior_mean, prior_std)\n",
    "                cur_instance = self.transfer_mlp(prior_instance, aim)\n",
    "                current_transfer_mean += cur_instance[0]\n",
    "                current_transfer_std += cur_instance[1]\n",
    "\n",
    "            # 取多个采样的Q(Zt-1)分布的均值为最终的loss 计算使用的P(Zt|B1:t-1)分布\n",
    "            current_transfer_mean = current_transfer_mean.div(self.sample_num)\n",
    "            current_transfer_std = current_transfer_std.div(self.sample_num)\n",
    "\n",
    "            kl_loss = self.DKL(current_mean, current_std, current_transfer_mean, current_transfer_std, True)\n",
    "        \n",
    "        return kl_loss\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    KL 误差\n",
    "    KL(Q(Zt)||P(Zt|B1:t-1))\n",
    "    P(Zt|B1:t-1) 使用采样计算～～1/K sum_{i=1}^K(P(Zt|Z_{i}t-1))\n",
    "    '''\n",
    "    def DKL(self, mean1, std1, mean2, std2, neg = False):\n",
    "        var1 = std1.pow(2)\n",
    "        var2 = std2.pow(2)\n",
    "        mean_pow2 = (mean2-mean1)*(torch.tensor(1.0)/var2)*(mean2-mean1)\n",
    "        tr_std_mul = (torch.tensor(1.0)/var2)*var1\n",
    "        if (neg == False):\n",
    "            dkl = (torch.log(var2/var1)-1+tr_std_mul+mean_pow2).mul(0.5).sum(dim=1).mean()\n",
    "        else:\n",
    "            dkl = (torch.log(var2/var1)-1+tr_std_mul+mean_pow2).mul(0.5).sum(dim=2).sum(dim=1).mean()\n",
    "        return dkl\n",
    "    \n",
    "    '''\n",
    "    主损失 重构误差\n",
    "    -Eq(log{P(Bt|Zt)})\n",
    "    '''\n",
    "    def lossF_sigmod_ml(self, user, query, item_pos, items_neg):\n",
    "        u_plus_q = user+query\n",
    "        dis_pos = (u_plus_q - item_pos).norm(2, dim=1).mul(5.0)\n",
    "        u_plus_q.unsqueeze_(1)\n",
    "        dis_neg = (u_plus_q - items_neg)\n",
    "        dis_neg = dis_neg.norm(2,dim=2)\n",
    "        dis_pos = dis_pos.view(-1,1)\n",
    "        batch_loss = torch.log(torch.sigmoid(dis_neg-dis_pos)).sum(dim=1)*(-1.0)\n",
    "        return batch_loss.mean() , dis_pos.mean(), dis_neg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'Musical_Instruments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input_data/dataset_time_'+data_name+'.bin', 'rb') as f:\n",
    "    data_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "data_gen = DataLoader(data_set, batch_size=2, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 1429, 15922, 10251)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dataset statistic\n",
    "'''\n",
    "data_set.productNum, data_set.userNum, data_set.wordNum, len(data_set.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "实验参数\n",
    "'''\n",
    "embedding_dim = 50\n",
    "out_size = 10\n",
    "batch_size = 2\n",
    "neg_sample_num = data_set.neg_sample_num\n",
    "dataLen = len(data_set.train_data)\n",
    "batch_num = int(dataLen/batch_size)\n",
    "full_len = batch_num*batch_size\n",
    "time_bin_num = len(data_set.time_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = DataLoader(data_set, batch_size=2, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(data_gen)\n",
    "data = it.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(uid, pid_pos, qids_pos, len_pos, text_ids, len_r, time_bin_pos)\\nneg_item,neg_word\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(uid, pid_pos, qids_pos, len_pos, text_ids, len_r, time_bin_pos)\n",
    "neg_item,neg_word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1121,  794])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbml = PSM(data_set.userNum, data_set.productNum, data_set.wordNum, embedding_dim,\\\n",
    "                 data_set.max_query_len, data_set.max_review_len, batch_size, data_set.time_num+1,\\\n",
    "                 neg_num=5,sample_num=1,transfer_hidden_dim=100,\\\n",
    "                 sigma_parameter=1e0, kl_parameter=1e0, word_parameter=1e0, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([5449, 5449]),\n",
       " tensor([11709, 11709]),\n",
       " tensor([13290,  4970]),\n",
       " tensor([ 5419, 15377]),\n",
       " tensor([ 5644, 15359]),\n",
       " tensor([13100, 11310]),\n",
       " tensor([ 750, 2574]),\n",
       " tensor([14889, 11343]),\n",
       " tensor([14311,     0]),\n",
       " tensor([0, 0]),\n",
       " tensor([0, 0]),\n",
       " tensor([0, 0])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####@@####\n",
      "torch.Size([2, 47, 50])\n",
      "torch.Size([2, 47, 50])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5]) torch.Size([2, 47])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (47) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-bf54184b32eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muser_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_sample_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mword_mean_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_std_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_mean_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_std_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-d7a2bfc36215>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, user, item_pos, query, query_len, word, word_len, times, items_neg, word_neg)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mloss_main\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossF_sigmod_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_sample_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# 计算uw和iw的损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0muser_word_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_sample_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mitem_word_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_sample_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-d7a2bfc36215>\u001b[0m in \u001b[0;36mword_loss\u001b[0;34m(self, itemOrUser, word_pos, word_len, word_neg)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemOrUser\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdis_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_neg\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdis_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mwl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_neg\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdis_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mitemOrUser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (47) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "user_mean, user_std, query, \\\n",
    "item_mean_pos, item_std_pos,\\\n",
    "items_mean_neg, items_std_neg, \\\n",
    "user_sample, product_sample, product_sample_neg, \\\n",
    "loss, dis_pos, dis_neg,\\\n",
    "word_mean_pos, word_std_pos, word_mean_neg, word_std_neg = dbml(data[0][0], data[0][1], torch.stack(data[0][2]).t(), data[0][3], torch.stack(data[0][4]).t(), data[0][5], data[0][6], torch.stack(data[1][0]).t(), torch.stack(data[1][1]).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 50]),\n",
       " torch.Size([2, 50]),\n",
       " torch.Size([2, 50]),\n",
       " torch.Size([2, 5, 50]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user_mean.shape,user_std.shape,query.shape,item_mean_pos.shape,item_std_pos.shape\n",
    "items_mean_neg.shape, user_sample.shape, product_sample.shape, product_sample_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1119.6719, grad_fn=<AddBackward0>),\n",
       " tensor(50.6291, grad_fn=<MeanBackward0>),\n",
       " tensor(10.5670, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,dis_pos,dis_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 47, 50]),\n",
       " torch.Size([2, 47, 50]),\n",
       " torch.Size([2, 5, 50]),\n",
       " torch.Size([2, 5, 50]))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_mean_pos.shape, word_std_pos.shape, word_mean_neg.shape, word_std_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
